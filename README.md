# ğŸ¤Ÿ Sign Language Detection using Deep Learning & OpenCV

This project detects **sign language gestures** using a live webcam feed and classifies them into predefined actions using a custom-trained deep learning model.

---

## ğŸ“Œ Overview

The system uses **MediaPipe**, **OpenCV**, and a deep learning model (trained with Keras) to:
- ğŸ“¹ Capture real-time video input
- ğŸ– Track hand and body landmarks
- ğŸ§  Classify sequences into specific sign language gestures

---

## ğŸ§° Tech Stack

- Python
- OpenCV
- MediaPipe
- NumPy
- TensorFlow / Keras

---

## ğŸ§  Model Info

- Trained using `.npy` files generated from real-time MediaPipe landmark tracking
- Model saved as `action.h5`
- Supports classification of multiple sign actions like: `hello`, `thanks`, `iloveyou`, etc.

---

## ğŸ“ Project Structure

signLanguageDetection/
â”œâ”€â”€ Sign Language Detection.ipynb     # Jupyter notebook with training + prediction
â”œâ”€â”€ action.h5                         # Trained gesture recognition model
â”œâ”€â”€ MP_Data/                          # Raw MediaPipe data samples for training
â”œâ”€â”€ Logs/
â”‚   â””â”€â”€ train/                        # TensorBoard training logs
â”œâ”€â”€ 0.npy                             # Example gesture training array (NumPy)
â”œâ”€â”€ .ipynb_checkpoints/               # Jupyter autosaved checkpoints

